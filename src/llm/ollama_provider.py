"""
Ollama Provider - —Ä–∞–±–æ—Ç–∞ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç raw socket –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å 503
"""

import socket
import json
import asyncio
from typing import List, Optional

from src.llm.base import BaseLLMProvider, Message, LLMResponse, LLMProviderError


class OllamaProvider(BaseLLMProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏).
    """

    def __init__(
            self,
            model: str = "qwen2.5:7b",
            temperature: float = 0.4,
            max_tokens: int = 2000,
            host: str = "127.0.0.1",
            port: int = 11434,
            timeout: int = 120,
            **kwargs
    ):
        super().__init__(model, temperature, max_tokens, **kwargs)
        self.host = host
        self.port = port
        self.timeout = timeout

    def _raw_request(self, method: str, path: str, body: dict = None) -> dict:
        """
        –î–µ–ª–∞–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ raw socket.
        –û–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—É 503 —Å httpx/requests.
        """
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(self.timeout)

        try:
            sock.connect((self.host, self.port))

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            if body:
                body_str = json.dumps(body)
                request = (
                    f"{method} {path} HTTP/1.1\r\n"
                    f"Host: {self.host}:{self.port}\r\n"
                    f"Content-Type: application/json\r\n"
                    f"Content-Length: {len(body_str)}\r\n"
                    f"Connection: close\r\n"
                    f"\r\n"
                    f"{body_str}"
                )
            else:
                request = (
                    f"{method} {path} HTTP/1.1\r\n"
                    f"Host: {self.host}:{self.port}\r\n"
                    f"Connection: close\r\n"
                    f"\r\n"
                )

            sock.send(request.encode())

            # –ß–∏—Ç–∞–µ–º –æ—Ç–≤–µ—Ç
            response = b""
            while True:
                data = sock.recv(4096)
                if not data:
                    break
                response += data

            # –ü–∞—Ä—Å–∏–º HTTP –æ—Ç–≤–µ—Ç
            response_str = response.decode('utf-8', errors='replace')

            # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç–µ–ª–æ
            header_end = response_str.find("\r\n\r\n")
            if header_end == -1:
                raise LLMProviderError("Invalid HTTP response")

            headers = response_str[:header_end]
            body_text = response_str[header_end + 4:]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
            first_line = headers.split("\r\n")[0]
            status_code = int(first_line.split()[1])

            if status_code != 200:
                raise LLMProviderError(f"HTTP {status_code}: {body_text[:200]}")

            # –ü–∞—Ä—Å–∏–º JSON
            return json.loads(body_text)

        finally:
            sock.close()

    async def generate(
            self,
            messages: List[Message],
            system_prompt: Optional[str] = None
    ) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama API."""

        ollama_messages = []

        if system_prompt:
            ollama_messages.append({
                "role": "system",
                "content": system_prompt
            })

        for msg in messages:
            ollama_messages.append(msg.to_dict())

        payload = {
            "model": self.model,
            "messages": ollama_messages,
            "stream": False,
            "options": {
                "temperature": self.temperature,
                "num_predict": self.max_tokens,
            }
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ thread pool —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        loop = asyncio.get_event_loop()
        data = await loop.run_in_executor(
            None,
            lambda: self._raw_request("POST", "/api/chat", payload)
        )

        content = data.get("message", {}).get("content", "")

        if not content:
            raise LLMProviderError("Empty response from Ollama")

        return LLMResponse(content=content, raw_response=data)

    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ Ollama –∏ –º–æ–¥–µ–ª—å."""
        try:
            data = self._raw_request("GET", "/api/tags")
            models = [m["name"] for m in data.get("models", [])]
            return self.model in models
        except:
            return False

    async def list_models(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        loop = asyncio.get_event_loop()
        data = await loop.run_in_executor(
            None,
            lambda: self._raw_request("GET", "/api/tags")
        )
        return [m["name"] for m in data.get("models", [])]

    async def close(self):
        """–ù–∏—á–µ–≥–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ."""
        pass


async def create_ollama_provider(
        model: str = "qwen2.5:7b",
        **kwargs
) -> OllamaProvider:
    """–°–æ–∑–¥–∞—ë—Ç Ollama –ø—Ä–æ–≤–∞–π–¥–µ—Ä."""
    provider = OllamaProvider(model=model, **kwargs)

    if not provider.is_available():
        raise LLMProviderError(
            f"Model {model} not found. Install with: ollama pull {model}"
        )

    return provider


# –¢–µ—Å—Ç
if __name__ == "__main__":
    import asyncio

    async def test_ollama():
        """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

        print("="*80)
        print("OLLAMA PROVIDER TEST")
        print("="*80)

        # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        print("\nüìç Creating provider...")
        try:
            provider = await create_ollama_provider(
                model="qwen2.5:7b",
                auto_pull=True,
                temperature=0.7
            )
            print(f"‚úÖ Provider ready: {provider}")
        except LLMProviderError as e:
            print(f"‚ùå Error: {e}")
            print("\nMake sure Ollama is running:")
            print("  brew install ollama")
            print("  ollama serve")
            return

        # –¢–µ—Å—Ç 1: –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        print("\n" + "‚îÄ"*80)
        print("TEST 1: Simple question")
        print("‚îÄ"*80)

        response = await provider.generate_simple(
            user_message="What is 2+2? Answer in one sentence.",
            system_prompt="You are a helpful AI assistant."
        )

        print(f"\nü§ñ Response: {response.content}")

        # –¢–µ—Å—Ç 2: JSON –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–≤–∞–∂–Ω–æ –¥–ª—è –∞–≥–µ–Ω—Ç–∞!)
        print("\n" + "‚îÄ"*80)
        print("TEST 2: JSON generation")
        print("‚îÄ"*80)

        response = await provider.generate_simple(
            user_message="""–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π JSON –æ–±—ä–µ–∫—Ç —Å —ç—Ç–∏–º–∏ –ø–æ–ª—è–º–∏:
- –∏–º—è: —Å–ª—É—á–∞–π–Ω–æ–µ –∏–º—è —á–µ–ª–æ–≤–µ–∫–∞
- –≤–æ–∑—Ä–∞—Å—Ç: —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç 20-50
- —Ö–æ–±–±–∏: —Å–ª—É—á–∞–π–Ω–æ–µ —Ö–æ–±–±–∏

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥—Ä—É–≥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.""",
            system_prompt="–¢—ã json –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ json."
        )

        print(f"\nü§ñ Response:\n{response.content}")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–∏–ª–∞ —Ç–µ–∫—Å—Ç)
            content = response.content.strip()
            start = content.find('{')
            end = content.rfind('}') + 1

            if 0 <= start < end:
                json_str = content[start:end]
                data = json.loads(json_str)
                print(f"‚úÖ Valid JSON parsed: {data}")
            else:
                print("‚ö†Ô∏è  No JSON found in response")
        except json.JSONDecodeError as e:
            print(f"‚ùå Invalid JSON: {e}")

        # –¢–µ—Å—Ç 3: –î–∏–∞–ª–æ–≥ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        print("\n" + "‚îÄ"*80)
        print("TEST 3: Conversation with history")
        print("‚îÄ"*80)

        messages = [
            Message(role="user", content="My name is Alex"),
            Message(role="assistant", content="Nice to meet you, Alex!"),
            Message(role="user", content="What's my name?")
        ]

        response = await provider.generate(
            messages=messages,
            system_prompt="You are a helpful assistant with good memory."
        )

        print(f"\nü§ñ Response: {response.content}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º
        await provider.close()
        print("\n‚úÖ All tests completed!")

    asyncio.run(test_ollama())