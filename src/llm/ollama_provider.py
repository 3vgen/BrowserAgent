"""
Ollama Provider - —Ä–∞–±–æ—Ç–∞ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama
"""

import httpx
from typing import List, Optional, Dict, Any
import json

from src.llm.base import BaseLLMProvider, Message, LLMResponse, LLMProviderError


class OllamaProvider(BaseLLMProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏).

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏:
    - qwen2.5:7b (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    - qwen2.5:14b (—É–º–Ω–µ–µ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
    - llama3.1
    - mistral
    - –∏ –¥—Ä—É–≥–∏–µ
    """

    def __init__(
        self,
        model: str = "qwen2.5:7b",
        temperature: float = 0.4,
        max_tokens: int = 2000,
        base_url: str = "http://localhost:11434",
        timeout: int = 120,
        **kwargs
    ):
        """
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ (–ø–∞—Ä–∞–º–µ—Ç—Ä num_predict –≤ Ollama)
            base_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        super().__init__(model, temperature, max_tokens, **kwargs)
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        transport = httpx.AsyncHTTPTransport(http1=True, http2=False)
        self.client = httpx.AsyncClient(transport=transport, timeout=timeout)

    async def generate(
        self,
        messages: List[Message],
        system_prompt: Optional[str] = None
    ) -> LLMResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama API.

        Ollama API –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç:
        POST /api/chat
        {
          "model": "qwen2.5:7b",
          "messages": [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."}
          ],
          "stream": false
        }
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        ollama_messages = []

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
        if system_prompt:
            ollama_messages.append({
                "role": "system",
                "content": system_prompt
            })

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        for msg in messages:
            ollama_messages.append(msg.to_dict())

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        payload = {
            "model": self.model,
            "messages": ollama_messages,
            "stream": False,  # –ù–µ —Å—Ç—Ä–∏–º–∏–º –ø–æ–∫–∞
            "options": {
                "temperature": self.temperature,
                "num_predict": self.max_tokens,
            }
        }

        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = await self.client.post(
                f"{self.base_url}/api/chat",
                json=payload
            )
            response.raise_for_status()

            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            data = response.json()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            content = data.get("message", {}).get("content", "")

            if not content:
                raise LLMProviderError("Empty response from Ollama")

            return LLMResponse(
                content=content,
                raw_response=data
            )

        except httpx.ConnectError:
            raise LLMProviderError(
                "Cannot connect to Ollama. Is it running? "
                "Start with: ollama serve"
            )
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                raise LLMProviderError(
                    f"Model '{self.model}' not found. "
                    f"Pull it with: ollama pull {self.model}"
                )
            raise LLMProviderError(f"Ollama HTTP error: {e}")
        except Exception as e:
            raise LLMProviderError(f"Ollama error: {e}")

    def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ Ollama —Å–µ—Ä–≤–µ—Ä.

        Returns:
            True –µ—Å–ª–∏ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
        """
        try:
            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            with httpx.Client(timeout=5) as client:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω
                response = client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]

                return self.model in models
        except:
            return False

    async def list_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
        """
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            data = response.json()

            return [m["name"] for m in data.get("models", [])]
        except Exception as e:
            raise LLMProviderError(f"Cannot list models: {e}")

    async def pull_model(self, model: str) -> None:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞).

        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        print(f"üì• Downloading model: {model}")
        print("This may take a few minutes...")

        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/api/pull",
                json={"name": model},
                timeout=None  # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–∏–º
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            status = data.get("status", "")

                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                            if "total" in data and "completed" in data:
                                total = data["total"]
                                completed = data["completed"]
                                percent = (completed / total * 100) if total > 0 else 0
                                print(f"\r{status}: {percent:.1f}%", end="", flush=True)
                            else:
                                print(f"\r{status}", end="", flush=True)
                        except json.JSONDecodeError:
                            pass

            print(f"\n‚úÖ Model {model} ready!")

        except Exception as e:
            raise LLMProviderError(f"Cannot pull model: {e}")

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        await self.client.aclose()


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
async def create_ollama_provider(
    model: str = "qwen2.5:7b",
    auto_pull: bool = True,
    **kwargs
) -> OllamaProvider:
    """
    –°–æ–∑–¥–∞—ë—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç Ollama –ø—Ä–æ–≤–∞–π–¥–µ—Ä.

    Args:
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        auto_pull: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π OllamaProvider
    """
    provider = OllamaProvider(model=model, **kwargs)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    if not provider.is_available():
        print(f"‚ö†Ô∏è  Model {model} not found locally")

        if auto_pull:
            await provider.pull_model(model)
        else:
            raise LLMProviderError(
                f"Model {model} not found. "
                f"Install with: ollama pull {model}"
            )

    return provider


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import asyncio

    async def test_ollama():
        """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

        print("="*80)
        print("OLLAMA PROVIDER TEST")
        print("="*80)

        # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        print("\nüìç Creating provider...")
        try:
            provider = await create_ollama_provider(
                model="qwen2.5:7b",
                auto_pull=True,
                temperature=0.7
            )
            print(f"‚úÖ Provider ready: {provider}")
        except LLMProviderError as e:
            print(f"‚ùå Error: {e}")
            print("\nMake sure Ollama is running:")
            print("  brew install ollama")
            print("  ollama serve")
            return

        # –¢–µ—Å—Ç 1: –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        print("\n" + "‚îÄ"*80)
        print("TEST 1: Simple question")
        print("‚îÄ"*80)

        response = await provider.generate_simple(
            user_message="What is 2+2? Answer in one sentence.",
            system_prompt="You are a helpful AI assistant."
        )

        print(f"\nü§ñ Response: {response.content}")

        # –¢–µ—Å—Ç 2: JSON –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–≤–∞–∂–Ω–æ –¥–ª—è –∞–≥–µ–Ω—Ç–∞!)
        print("\n" + "‚îÄ"*80)
        print("TEST 2: JSON generation")
        print("‚îÄ"*80)

        response = await provider.generate_simple(
            user_message="""–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π JSON –æ–±—ä–µ–∫—Ç —Å —ç—Ç–∏–º–∏ –ø–æ–ª—è–º–∏:
- –∏–º—è: —Å–ª—É—á–∞–π–Ω–æ–µ –∏–º—è —á–µ–ª–æ–≤–µ–∫–∞
- –≤–æ–∑—Ä–∞—Å—Ç: —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç 20-50
- —Ö–æ–±–±–∏: —Å–ª—É—á–∞–π–Ω–æ–µ —Ö–æ–±–±–∏

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥—Ä—É–≥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.""",
            system_prompt="–¢—ã json –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ json."
        )

        print(f"\nü§ñ Response:\n{response.content}")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–∏–ª–∞ —Ç–µ–∫—Å—Ç)
            content = response.content.strip()
            start = content.find('{')
            end = content.rfind('}') + 1

            if 0 <= start < end:
                json_str = content[start:end]
                data = json.loads(json_str)
                print(f"‚úÖ Valid JSON parsed: {data}")
            else:
                print("‚ö†Ô∏è  No JSON found in response")
        except json.JSONDecodeError as e:
            print(f"‚ùå Invalid JSON: {e}")

        # –¢–µ—Å—Ç 3: –î–∏–∞–ª–æ–≥ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        print("\n" + "‚îÄ"*80)
        print("TEST 3: Conversation with history")
        print("‚îÄ"*80)

        messages = [
            Message(role="user", content="My name is Alex"),
            Message(role="assistant", content="Nice to meet you, Alex!"),
            Message(role="user", content="What's my name?")
        ]

        response = await provider.generate(
            messages=messages,
            system_prompt="You are a helpful assistant with good memory."
        )

        print(f"\nü§ñ Response: {response.content}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º
        await provider.close()
        print("\n‚úÖ All tests completed!")

    asyncio.run(test_ollama())